from bs4 import BeautifulSoup
import requests
import pandas as pd
import time
from nltk.tokenize import sent_tokenize
import spacy
nlp = en_core_web_sm.load()

def content(link):
    
    """
        This function goes inside the link of every text and scraps data from the each news headline
        in parallel
        
    """
    try:
        # Requesting the Scraped Link
        time.sleep(2)     # Reducing number of pings to avoid ip blocks though not neccessary at this stage
        source = requests.get(link[1:], verify = False)
        
        # Check if the request is successful
        if source.status_code == 200:
            
            # Parsing the link text
            source = BeautifulSoup(source.text, 'lxml')
            context = ' '
            
            # Finding the Paragraph text using 'p' tag
            for data in source.find_all('p'):
                
                # Implementing a bit of cleaning text at initial stage
                if len(data) < 2:
                    data = data.text.replace('  ', '')
                    context = context + ' ' + data.rstrip().lstrip().rstrip('\n').lstrip('\n')                   
        else:
            
            print('Requested site for content did not respond. Status Code is ', source.status_code)
    except Exception as e:
        print(str(e))
            
    return context
        

def scrap_headline_window(name, days, keys):
    
    """
        This function hits the google page and scraps the 
    """

    # Defining a dataframe to store the data and later saving it to excel
    test_dataframe = pd.DataFrame(columns = ['News', 'Content', 'Link', 'Keyword'])
    
    # Putting it into try-catch to handle any error    
    try:
        
        # Defining a baseline URL 
            url= 'https://www.google.com/search?q={}+%22{}%22&rlz=1C1GGRV_enIN790IN790&tbm=nws&source=lnt&tbs=qdr:d{}&sa=X&ved=0ahUKEwioxqnMjqngAhURaVAKHb8rAUsQpwUIIQ&biw=1440&bih=758&dpr=1'.format(name, keys, days)
            
        # Hitting the URL and Parsing it
            source = requests.get(url)
            time.sleep(5)      # Reducing overload condition on the server else the IP might get blocked
            
            # Checking the Status code for successful request
            if source.status_code == 200:
                
                source = BeautifulSoup(source.text, 'lxml')
                # Scraping the news and the respective links
                for elements in source.find_all('div', class_ = 'g'):
                    data = []
                    heading = elements.find('h3').text
                    link = '.' + elements.find('a')['href'].split('=')[1][:-3]
                    context = content(link)
                    data.append([heading, context, link, keys])
                    sample_dataframe = pd.DataFrame(data = data, columns = ['News', 'Content', 'Link', 'Keyword'])
                    test_dataframe = pd.concat([test_dataframe, sample_dataframe], axis = 0)
                    print(data)
                    print()

            
            else:
                
                print('Request failed with status code', source.status_code())
                
    except Exception as e:
        
        print(str(e))
        pass
    
    # Writing to an Excel File 

#    test_dataframe.to_excel('D:\\Users\\kekishor\\Desktop\\project\\filtered_extract\\' + name + '.xlsx')    
    return test_dataframe

# This stopwords file can be enhanced more to enrich the data    
with open('stopwords.txt') as ads_word:
    
    """    
        The sentences containing these words will removed from the content we got        
    """   
    
    stopword = []
    for i in ads_word.readlines():
        if len(i) > 1:
            stopword.append(i)


# Implementing the Cleaning Process
def cleaning_text(content):
    
    """
    
        Cleaning the text process involves the following steps
            
            -- Implementation of striping spaces and new line
                    This step has been implemented in the function 'Content' while scraping the data
            -- Implementation of sentence tokenizer 
                    To process each sentence individually
            -- Implementation of stopwords
                    These keywords are manually prepared for the obvious keys which shows waste data
            -- Sentence Less than 4 words should be removed. High chances of being an ad
            
    """
    filtered_message = ''  # Resetting the filtered message for every token
    
    # Implementing the doc for using it to split using spacy
    
    try:          # Wrapping this because some of the content field is empty
        
        doc = nlp(content)
        for word in doc.sents:
            
            # Wrapping in try-catch to ensure exception handling
            try:
                
                # Since the word is of span class, thus type casting it to be a string
                word = str(word)
                spam = 0      # Setting the spam flag, whenever it is set to 1, the sentence will be discarded
                    
                for ad_word in stopword:     # Checking for stopword
                    if (ad_word in word.lower()):
                        spam = 1
                        break
                        
                if len(word.split())< 5:  # Checking for length of the sentence
                    spam = 1
                
                if spam == 0:     # If flag does not changes, then adding it to the filtered message
                    filtered_message = filtered_message + word + ' '
                    
            except Exception as e:
                
                print(str(e))
                pass
            
            print(filtered_message)
        return filtered_message
    
    except Exception as e:          # Returning space when the content is empty 
        
        print(str(e))
        print(content)
        return ' '





# Retrieving company names from the manually created files   
company_names = pd.read_excel(r'D:\Users\kekishor\Desktop\project\insurance_company names\insurance_companies.xlsx').iloc[:, 0] 

# Amount of window for which we need the data
days = int(input('Enter the Window for which we need to scrap the news '))

# Keys for which we need the data
keywords = ['ACCIDENT', 'PREMIUM' , 'SERVICES', 'PRODUCT', 'RISK', 'RATE', 'BROKER', 
            'BUSINESS', 'CANCELLATION', 'CLAIM', 'CLAUSE', 'COVERAGE', 'INSURANCE', 'FORGERY', 
            'GRACE PERIOD', 'INDEMNITY', 'INSURABILITY', 'LAPSE', 'LESSOR', 'LIABILITY', 'LOSS', 
            'MORTGAGE', 'POLICY', 'REIMBURSEMENT', 'REINSTATEMENT', 'RENEWAL', 'SETTLEMENT', 'VALUATION', 
            'WAIVER']

company_names = ['Allianz']
# Loop all the company names with each of these keywords (Might take a bit)
for company in company_names:
    final_data = pd.DataFrame(columns = ['News', 'Content', 'Link', 'Keyword'])
    for keys  in keywords:
        final_data = pd.concat([final_data, scrap_headline_window(company, days, keys)], axis = 0)
    final_data.set_index('News', inplace = True)
    
    # Process to write the data into the excel file as a backup for future purposes
    writer = pd.ExcelWriter(r'D:\\Users\\kekishor\\Desktop\\project\\filtered_extract\\' + company + '.xlsx', engine='xlsxwriter')
    final_data.to_excel(writer)
    workbook = writer.book
    worksheet = writer.sheets['Sheet1']
    wrap_format = workbook.add_format({'text_wrap' : True})
                
    worksheet.set_column('A:A', 70, wrap_format)
    worksheet.set_column('B:B', 100, wrap_format)
    worksheet.set_column('C:C', 80, wrap_format)
    worksheet.set_column('D:D', 20, wrap_format)
    worksheet.set_column('E:E', 40, wrap_format)
    worksheet.set_column('F:F', 40, wrap_format)
    
    writer.save()
    writer.close()
