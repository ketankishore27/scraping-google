from bs4 import BeautifulSoup
import requests
import pandas as pd
import time

def content(link):
    
    """
        This function goes inside the link of every text and scraps data from the each news headline
        in parallel
        
    """
    # Requesting the Scraped Link
    time.sleep(2)               # Reducing number of pings to avoid ip blocks though not neccessary at this stage
    source = requests.get(link[1:], verify = False)
    
    # Check if the request is successful
    if source.status_code == 200:
        
        # Parsing the link text
        source = BeautifulSoup(source.text, 'lxml')
        context = ' '
        
        # Finding the Paragraph text using 'p' tag
        for data in source.find_all('p'):
            context = context + ' ' +data.text
               
    else:
        
        print('Requested site for content did not respond. Status Code is ', source.status_code)
            
    return context
        

def scrap_headline_window(name, days, keys):
    
    """
        This function hits the google page and scraps the 
    """

    # Defining a dataframe to store the data and later saving it to excel
    test_dataframe = pd.DataFrame(columns = ['News', 'Content', 'Link', 'Keyword'])
    
    # Putting it into try-catch to handle any error    
    try:
        
        # Defining a baseline URL 
            url= 'https://www.google.com/search?q={}+%22{}%22&rlz=1C1GGRV_enIN790IN790&tbm=nws&source=lnt&tbs=qdr:d{}&sa=X&ved=0ahUKEwioxqnMjqngAhURaVAKHb8rAUsQpwUIIQ&biw=1440&bih=758&dpr=1'.format(name, keys, days)
            
        # Hitting the URL and Parsing it
            source = requests.get(url)
            time.sleep(3)      # Reducing overload condition on the server else the IP might get blocked
            
            # Checking the Status code for successful request
            if source.status_code == 200:
                
                source = BeautifulSoup(source.text, 'lxml')
                # Scraping the news and the respective links
                for elements in source.find_all('div', class_ = 'g'):
                    data = []
                    heading = elements.find('h3').text
                    link = '.' + elements.find('a')['href'].split('=')[1][:-3]
                    context = content(link)
                    data.append([heading, context, link, keys])
                    sample_dataframe = pd.DataFrame(data = data, columns = ['News', 'Content', 'Link', 'Keyword'])
                    test_dataframe = pd.concat([test_dataframe, sample_dataframe], axis = 0)
                    print(data)
                    print()

            
            else:
                
                print('Request failed with status code', source.status_code())
                
    except Exception as e:
        
        print(str(e))
        pass
    
    # Writing to an Excel File 

#    test_dataframe.to_excel('D:\\Users\\kekishor\\Desktop\\project\\filtered_extract\\' + name + '.xlsx')    
    return test_dataframe

# Retrieving company names from the manually created files   
company_names = pd.read_excel(r'D:\Users\kekishor\Desktop\project\insurance_company names\insurance_companies.xlsx').iloc[:, 0] 

# Amount of window for which we need the data
days = int(input('Enter the Window for which we need to scrap the news '))

# Keys for which we need the data
keywords = ['ACCIDENT', 'PREMIUM' , 'SERVICES', 'PRODUCT', 'RISK', 'RATE', 'BROKER', 
            'BUSINESS', 'CANCELLATION', 'CLAIM', 'CLAUSE', 'COVERAGE', 'INSURANCE', 'FORGERY', 
            'GRACE PERIOD', 'INDEMNITY', 'INSURABILITY', 'LAPSE', 'LESSOR', 'LIABILITY', 'LOSS', 
            'MORTGAGE', 'POLICY', 'REIMBURSEMENT', 'REINSTATEMENT', 'RENEWAL', 'SETTLEMENT', 'VALUATION', 
            'WAIVER']

# Loop all the company names with each of these keywords (Might take a bit)
for company in company_names:
    final_data = pd.DataFrame(columns = ['News', 'Content', 'Link', 'Keyword'])
    for keys  in keywords:
        final_data = pd.concat([final_data, scrap_headline_window(company, days, keys)], axis = 0)
    final_data.set_index('News', inplace = True)
    
    # Process to write the data into the excel file as a backup for future purposes
    writer = pd.ExcelWriter(r'D:\\Users\\kekishor\\Desktop\\project\\filtered_extract\\' + company + '.xlsx', engine='xlsxwriter')
    final_data.to_excel(writer)
    workbook = writer.book
    worksheet = writer.sheets['Sheet1']
    wrap_format = workbook.add_format({'text_wrap' : True})
                
    worksheet.set_column('A:A', 70, wrap_format)
    worksheet.set_column('B:B', 100, wrap_format)
    worksheet.set_column('C:C', 80, wrap_format)
    worksheet.set_column('D:D', 20, wrap_format)
    
    writer.save()
    writer.close()    
